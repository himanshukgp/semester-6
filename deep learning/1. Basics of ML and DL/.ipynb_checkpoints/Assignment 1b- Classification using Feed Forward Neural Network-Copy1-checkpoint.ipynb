{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T19:16:37.152729Z",
     "start_time": "2019-01-30T19:16:37.134208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Name : Himanshu\n",
    "Roll No: 16MA20020\n",
    "\n",
    "Assignment 1b\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('Iris_Data.csv')\n",
    "df.head()\n",
    "\n",
    "'''\n",
    "You will not import any other library other than these provided.\n",
    "\n",
    "We provide the iris_dataset for the classification task\n",
    "There are 4 dependent variables columns(1-4).\n",
    "The last column (category of the flower) is what we wish to predict\n",
    "\n",
    "The first part of this task is similar to assignment 1 a\n",
    "'''\n",
    "# reads the file and stores in 2 numpy arrays.\n",
    "# X has the input features and Y has the output value in numpy array\n",
    "\n",
    "X = df.iloc[:,:-1].values\n",
    "Y = df.iloc[:,-1].values\n",
    "\n",
    "rows,cols= X.shape[0], X.shape[1] #?, #? \n",
    "# how to get the number of rows and columns in the dataset.\n",
    "# Rows correspond to the number of input instances, columns correspond to the feature of an input\n",
    "\n",
    "print(rows,cols)\n",
    "\n",
    "np.random.seed(42) # to ensure that the same seed is generated\n",
    "\n",
    "# write code to shuffle the dataset\n",
    "\n",
    "def shuffle_dataset(X,Y):\n",
    "    \n",
    "    '''\n",
    "        Write code to shuffle the dataset here. \n",
    "        \n",
    "        Args: \n",
    "            X: Input feature ndarray\n",
    "            Y: Input values ndarray\n",
    "            \n",
    "        Return:\n",
    "            X and Y shuffled in place\n",
    "    \n",
    "    '''\n",
    "    arr = np.arange(rows)\n",
    "    np.random.shuffle(arr)\n",
    "    X = X[arr]\n",
    "    Y = Y[arr]\n",
    "    \n",
    "\n",
    "training_size = int(0.8*rows)\n",
    "X_train = np.nan_to_num(X[:training_size])\n",
    "y_train = np.nan_to_num(Y[:training_size])\n",
    "X_test = np.nan_to_num(X[training_size:])\n",
    "y_test = np.nan_to_num(Y[training_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T19:16:37.698945Z",
     "start_time": "2019-01-30T19:16:37.686411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Feed Forward Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T19:41:45.690601Z",
     "start_time": "2019-01-30T19:41:45.633257Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, input_no, hidden_no, output_no ):\n",
    "        '''\n",
    "            Initialize the Neural network model \n",
    "            Args:\n",
    "                input_no : no of input features (no of cols)\n",
    "                hidden_no: no of hidden nodes in the model\n",
    "                output_no: no of categories our model can classify\n",
    "                      \n",
    "        \n",
    "        '''\n",
    "        self.h= np.zeros(hidden_no) # Initialize the hidden layer with zero ?\n",
    "        self.w1= np.random.rand(hidden_no,cols) # Initialize the weights from the input to the hidden layer uniformly with values between 0 and 0.01  ?\n",
    "        #print(self.w1.shape)\n",
    "        self.b1= np.random.rand(hidden_no) # Initialize the biases uniformly with values between 0 and 0.01 equal to the number of hidden nodes  ?\n",
    "        self.w2= np.random.rand(output_no,hidden_no) # Initialize the weights from the hidden layer to the output uniformly with values between 0 and 0.01 ? \n",
    "        self.b2= np.random.rand(output_no)# Initialize the biases uniformly with values between 0 and 0.01 equal to the number of output categories ?\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            Do a forward pass on the NN model \n",
    "            Args: \n",
    "                x : Input feature matrix \n",
    "                \n",
    "            Return:\n",
    "                y_pred : list of predicted probabilities of x\n",
    "\n",
    "                h = relu(w1.x+b1) \n",
    "                y_pred = softmax(w2.h+b2)\n",
    "                \n",
    "        \n",
    "        '''\n",
    "        y_pred=[]\n",
    "        \n",
    "        # evaluate activations of first hidden layer\n",
    "        self.h = np.matmul(x,np.transpose(self.w1)) + self.b1\n",
    "        for i in range(self.h.shape[0]):\n",
    "            for j in range(self.h.shape[1]):\n",
    "                if self.h[i][j]<0:\n",
    "                    self.h[i][j]=0\n",
    "        \n",
    "        y_pred = np.matmul(self.h, np.transpose(self.w2)) + self.b2\n",
    "        sum_ = np.sum(np.exp(y_pred), axis=1)\n",
    "        #print(sum_.shape)\n",
    "        y_pred = np.divide(np.exp(y_pred), (sum_.reshape(-1,1)))\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def backward(self, x, y_train, y_pred, lr):\n",
    "        \n",
    "        '''\n",
    "            Do a backward pass on the NN model. \n",
    "            Computes all gradients and updates the parameters w1, b1, w2, b2\n",
    "            \n",
    "            Args:\n",
    "                x: input matrix X \n",
    "                y_train: actual category of the feature/ data point\n",
    "                y_pred: predicted probabilities of the categories obtained during forward pass\n",
    "                lr: learning rate        \n",
    "        '''\n",
    "        \n",
    "        y_ = np.zeros((y_train.shape[0], 3))\n",
    "        for i in range(y_.shape[0]):\n",
    "            y_[y_train[i]]=1\n",
    "        y_train = y_\n",
    "        \n",
    "        H_in = np.matmul(x,np.transpose(self.w1)) + self.b1\n",
    "        h = H_in\n",
    "        for i in range(h.shape[0]):\n",
    "            for j in range(h.shape[1]):\n",
    "                if h[i][j]<0:\n",
    "                    h[i][j]=0\n",
    "        H_out = h\n",
    "        O_in = np.matmul(h,np.transpose(self.w2)) + self.b2\n",
    "        O_out = self.forward(x)\n",
    "        \n",
    "        dE_dout = -1 * ( np.multiply(y_train, 1/y_pred) + np.multiply((1-y_train), 1/(1-y_pred)) )\n",
    "        \n",
    "        dOout_dOin = O_in\n",
    "        for i in range(O_in.shape[1]):\n",
    "            dOout_dOin[:,i] = (O_in[:,i]*(np.sum(O_in, axis=1)-O_in[:,i]))/np.power(np.sum(O_in, axis=1), 2)\n",
    "        \n",
    "        print(dOout_dOin)\n",
    "        dW2_1 = np.zeros(self.w2.shape)\n",
    "        db2_1 = np.zeros(self.b2.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            temp = np.ones(self.w2.shape)\n",
    "            temp = temp*H_out[i]\n",
    "            temp = temp*dE_dout[i].reshape(-1,1)\n",
    "            temp = temp*dOout_dOin[i].reshape(-1,1)\n",
    "            dW2_1 += temp\n",
    "            \n",
    "            db2_1 += dE_dout[i]*dOout_dOin[i]\n",
    "        \n",
    "        dW2 = dW2_1/x.shape[0]\n",
    "        db2 = db2_1/x.shape[0]\n",
    "        \n",
    "        \n",
    "        dHout_dHin = (H_in > 0).astype(int)\n",
    "        \n",
    "        temp1 = dE_dout*dOout_dOin\n",
    "        temp2 = np.ones((x.shape[0], self.w2.shape[1], self.w2.shape[0]))\n",
    "        #temp2 = temp2*temp1\n",
    "        for i in range(x.shape[0]):\n",
    "            temp2[i] = temp2[i]*temp1[i]\n",
    "            temp2[i] = np.multiply(temp2[i], np.transpose(self.w2))\n",
    "        temp2 = np.sum(temp2, axis=2)\n",
    "        \n",
    "        dEtot_dHout = temp2\n",
    "        \n",
    "        dW1_1 = np.zeros(self.w1.shape)\n",
    "        db2_1 = np.zeros(self.b1.shape)\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            temp = np.ones(self.w1.shape)\n",
    "            temp = temp*dHout_dHin[i].reshape(-1,1)\n",
    "            temp = temp*x[i]\n",
    "            temp = temp*dEtot_dHout[i].reshape(-1,1)\n",
    "            dW1_1 += temp\n",
    "            \n",
    "            db2_1 += dHout_dHin[i]*dEtot_dHout[i]\n",
    "            \n",
    "            \n",
    "        \n",
    "        dW1 = dW1_1/x.shape[0]\n",
    "        db1 = db2_1/x.shape[0]\n",
    "        \n",
    "        self.w1 = self.w1-dW1*lr\n",
    "        self.w2 = self.w2-dW2*lr\n",
    "        self.b1 = self.b1-db1*lr\n",
    "        self.b2 = self.b2-db2*lr\n",
    "        \n",
    "        #print(self.w1)\n",
    "        \n",
    "        \n",
    "def crossEntropy_loss(y_pred, y_train):\n",
    "    '''\n",
    "        Computes the cross entropy loss between the predicted values and the actual values\n",
    "        \n",
    "        Args:\n",
    "            y_pred: predicted probabilities of the categories obtained during forward pass\n",
    "            y_train: actual category of the feature/ data point\n",
    "    \n",
    "    '''\n",
    "    y_ = np.zeros((y_train.shape[0], 3))\n",
    "    for i in range(y_.shape[0]):\n",
    "        y_[y_train[i]]=1\n",
    "    y_train = y_\n",
    "        \n",
    "    res = (-1) * ( (y_train * np.log(y_pred)) + ((1-y_train) * np.log(1-y_pred)) )\n",
    "    res = np.sum(res)/y_pred.shape[0]\n",
    "    return res\n",
    "\n",
    "def accuracy(y_pred,y_train):\n",
    "    '''\n",
    "        Computes the accuracy between the predicted values and actual labels\n",
    "    \n",
    "        Args:\n",
    "            y_pred: predicted probabilities of the categories obtained during forward pass\n",
    "            y_train: actual category of the feature/ data point\n",
    "\n",
    "    '''\n",
    "    \n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    res = np.sum((y_pred==y_train).astype(int))/y_pred.shape[0]\n",
    "    return res\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T19:41:46.300347Z",
     "start_time": "2019-01-30T19:41:46.223673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20908768 0.2465443  0.02503723]\n",
      " [0.20917487 0.24677441 0.02623534]\n",
      " [0.20931896 0.24672338 0.02669486]\n",
      " [0.20914212 0.24687933 0.02645113]\n",
      " [0.20910535 0.24654072 0.02510055]\n",
      " [0.20860484 0.2464927  0.02274472]\n",
      " [0.20915104 0.24674093 0.02603205]\n",
      " [0.20902039 0.24665159 0.02510619]\n",
      " [0.20932335 0.24697683 0.02760705]\n",
      " [0.20913583 0.24677313 0.02601807]\n",
      " [0.20891151 0.24644146 0.02389107]\n",
      " [0.20896989 0.24675527 0.02523947]\n",
      " [0.20927594 0.24679639 0.02672659]\n",
      " [0.20977816 0.24682913 0.0290782 ]\n",
      " [0.20907966 0.24610378 0.02351866]\n",
      " [0.20869976 0.24618611 0.02211682]\n",
      " [0.20895684 0.24631483 0.02372618]\n",
      " [0.20902004 0.24655646 0.02480856]\n",
      " [0.20861145 0.24643243 0.02254077]\n",
      " [0.20890785 0.24650549 0.02412031]\n",
      " [0.20874407 0.24662199 0.02376959]\n",
      " [0.20884916 0.24654889 0.02404124]\n",
      " [0.20963843 0.2464681  0.02725425]\n",
      " [0.20862209 0.2467692  0.02384025]\n",
      " [0.20868377 0.24687766 0.02439916]\n",
      " [0.20894672 0.24682839 0.02540746]\n",
      " [0.2087926  0.24671576 0.02437718]\n",
      " [0.20896741 0.24656047 0.02455329]\n",
      " [0.20907009 0.24654785 0.02497424]\n",
      " [0.20900659 0.24685431 0.02575692]\n",
      " [0.20898856 0.24685703 0.02569025]\n",
      " [0.20880028 0.24655902 0.02386778]\n",
      " [0.20899713 0.24635884 0.02395167]\n",
      " [0.20895345 0.24621738 0.02331315]\n",
      " [0.20913583 0.24677313 0.02601807]\n",
      " [0.20934101 0.24658216 0.02631348]\n",
      " [0.20908812 0.24638411 0.02450001]\n",
      " [0.20913583 0.24677313 0.02601807]\n",
      " [0.20942633 0.2468961  0.02777808]\n",
      " [0.20899665 0.24662196 0.02489831]\n",
      " [0.20914256 0.24654003 0.02530271]\n",
      " [0.20938533 0.24713505 0.02850735]\n",
      " [0.20940649 0.24682213 0.02742506]\n",
      " [0.20866232 0.24670413 0.02381505]\n",
      " [0.20849196 0.24668461 0.02291147]\n",
      " [0.20912874 0.24681646 0.0262108 ]\n",
      " [0.20887932 0.24653773 0.02407009]\n",
      " [0.20923993 0.24680059 0.02660808]\n",
      " [0.20893346 0.24646891 0.02408244]\n",
      " [0.20912653 0.24664045 0.02554648]\n",
      " [0.2062449  0.24724527 0.01538159]\n",
      " [0.20630414 0.2473281  0.01600635]\n",
      " [0.20612214 0.24732884 0.0152139 ]\n",
      " [0.2065578  0.24763465 0.01838774]\n",
      " [0.20622752 0.24741416 0.0160417 ]\n",
      " [0.20633197 0.24756616 0.01709834]\n",
      " [0.20619162 0.24737047 0.01570973]\n",
      " [0.20713099 0.247611   0.02069328]\n",
      " [0.20629386 0.24736696 0.01608653]\n",
      " [0.20661216 0.24758885 0.01844149]\n",
      " [0.20697021 0.24774015 0.02055782]\n",
      " [0.20643403 0.24741744 0.01695415]\n",
      " [0.20664859 0.24753359 0.01827889]\n",
      " [0.20621497 0.24749522 0.01630613]\n",
      " [0.20682183 0.24737294 0.01842241]\n",
      " [0.20637281 0.24726364 0.01602143]\n",
      " [0.20628563 0.24754518 0.01684828]\n",
      " [0.20663195 0.24747998 0.01796743]\n",
      " [0.20622355 0.24759349 0.01680786]\n",
      " [0.20669818 0.24753653 0.0185234 ]\n",
      " [0.2060781  0.24749772 0.01580093]\n",
      " [0.20658508 0.24737116 0.01738064]\n",
      " [0.2060633  0.24757892 0.01603941]\n",
      " [0.20626618 0.2475144  0.01656792]\n",
      " [0.20643729 0.24734684 0.01663023]\n",
      " [0.2063656  0.24730612 0.01616752]\n",
      " [0.20616799 0.2473894  0.01565491]\n",
      " [0.20601552 0.24741261 0.01514541]\n",
      " [0.20627939 0.24747936 0.01654227]\n",
      " [0.20697344 0.24739296 0.01909934]\n",
      " [0.20675162 0.2475653  0.01887996]\n",
      " [0.20684895 0.24754192 0.01917851]\n",
      " [0.20667148 0.24744034 0.01802384]\n",
      " [0.20595817 0.24763598 0.01584956]\n",
      " [0.20628476 0.24759033 0.01703908]\n",
      " [0.20628884 0.24737083 0.01613494]\n",
      " [0.20620739 0.24732888 0.01558781]\n",
      " [0.20634276 0.24752619 0.01698769]\n",
      " [0.20655343 0.2474603  0.01760808]\n",
      " [0.20657261 0.24758481 0.0182326 ]\n",
      " [0.20639952 0.24763949 0.01768874]\n",
      " [0.20627005 0.24745344 0.01636604]\n",
      " [0.20660992 0.24748654 0.01795124]\n",
      " [0.2071199  0.24761144 0.02065017]\n",
      " [0.20647855 0.24755306 0.01768231]\n",
      " [0.20653304 0.24745649 0.01747898]\n",
      " [0.20649263 0.24748206 0.01743713]\n",
      " [0.20643832 0.24739049 0.01681615]\n",
      " [0.20728172 0.24745661 0.02073722]\n",
      " [0.20653779 0.24748486 0.01764864]\n",
      " [0.20545636 0.24761009 0.01371683]\n",
      " [0.20587261 0.24768003 0.0157393 ]\n",
      " [0.20558269 0.24750044 0.01371069]\n",
      " [0.20572527 0.24761885 0.01479926]\n",
      " [0.2055732  0.24759611 0.0141134 ]\n",
      " [0.20536535 0.24752205 0.01285505]\n",
      " [0.20617477 0.24782589 0.01766519]\n",
      " [0.20550364 0.24754289 0.01349766]\n",
      " [0.20562396 0.24765209 0.01451653]\n",
      " [0.20548936 0.24741322 0.01297952]\n",
      " [0.20591016 0.24743553 0.01483961]\n",
      " [0.20580624 0.24759026 0.01505093]\n",
      " [0.20571962 0.24748939 0.0142624 ]\n",
      " [0.20586773 0.24772802 0.01596089]\n",
      " [0.20575342 0.24766139 0.01524138]\n",
      " [0.20575481 0.24749501 0.01447734]\n",
      " [0.20578079 0.24754291 0.01470354]\n",
      " [0.20539242 0.24738611 0.01237637]\n",
      " [0.20519909 0.24761917 0.01260973]\n",
      " [0.20598499 0.24772772 0.01636348]]\n",
      "[[0.05659968 0.05391408 0.00074476]\n",
      " [0.05661155 0.05395018 0.00078709]\n",
      " [0.05662148 0.05394622 0.00080591]\n",
      " [0.05661643 0.05397566 0.00080454]\n",
      " [0.05660195 0.05391603 0.00074937]\n",
      " [0.05656632 0.05390973 0.00067876]\n",
      " [0.05661258 0.05395547 0.00079428]\n",
      " [0.05659944 0.05393318 0.00075093]\n",
      " [0.05663181 0.05399347 0.00084641]\n",
      " [0.05661095 0.05395066 0.00077744]\n",
      " [0.05658458 0.05389602 0.00070402]\n",
      " [0.0566015  0.05395483 0.00076197]\n",
      " [0.05662052 0.05395417 0.00080067]\n",
      " [0.05665761 0.05396594 0.00088617]\n",
      " [0.05658281 0.05383904 0.0006804 ]\n",
      " [0.05656278 0.05385997 0.00065129]\n",
      " [0.05658177 0.05387802 0.00070388]\n",
      " [0.05659476 0.05391721 0.00074182]\n",
      " [0.05656378 0.05389415 0.00066285]\n",
      " [0.05658782 0.05391264 0.00072199]\n",
      " [0.05657899 0.05392405 0.00070282]\n",
      " [0.05658393 0.05391989 0.00072337]\n",
      " [0.0566358  0.05390704 0.00082168]\n",
      " [0.05657428 0.05395599 0.00072348]\n",
      " [0.0565883  0.05397855 0.00073974]\n",
      " [0.05659885 0.05395969 0.00076162]\n",
      " [0.05658528 0.05394735 0.00073775]\n",
      " [0.05659222 0.05391611 0.00072882]\n",
      " [0.05659744 0.05391214 0.00074021]\n",
      " [0.05660693 0.05397151 0.00078109]\n",
      " [0.05660454 0.05396912 0.00077608]\n",
      " [0.05657775 0.05391423 0.00071063]\n",
      " [0.05659166 0.05388844 0.00070697]\n",
      " [0.05658172 0.05386332 0.0006834 ]\n",
      " [0.05661095 0.05395066 0.00077744]\n",
      " [0.05661536 0.05391675 0.00078349]\n",
      " [0.05659155 0.05388191 0.00071714]\n",
      " [0.05661095 0.05395066 0.00077744]\n",
      " [0.05663589 0.05397934 0.00085014]\n",
      " [0.05659624 0.0539267  0.0007419 ]\n",
      " [0.05660241 0.05391516 0.00075833]\n",
      " [0.0566353  0.05401326 0.00087557]\n",
      " [0.05663324 0.05396872 0.00083879]\n",
      " [0.05657488 0.05394882 0.00072776]\n",
      " [0.05656727 0.05394671 0.00069303]\n",
      " [0.05660981 0.05396054 0.0007939 ]\n",
      " [0.0565884  0.05391766 0.00071794]\n",
      " [0.05662024 0.05396216 0.00080791]\n",
      " [0.05658748 0.0539018  0.00071215]\n",
      " [0.05660494 0.05392936 0.00076302]\n",
      " [0.05643299 0.05404528 0.00047069]\n",
      " [0.05643867 0.05406918 0.00049843]\n",
      " [0.05642747 0.05406407 0.00046905]\n",
      " [0.05646563 0.05413308 0.00058399]\n",
      " [0.0564359  0.05408241 0.00049905]\n",
      " [0.05645139 0.05412227 0.00054045]\n",
      " [0.05643286 0.05408187 0.0004927 ]\n",
      " [0.05650515 0.05413079 0.00066004]\n",
      " [0.05644084 0.05406999 0.0004957 ]\n",
      " [0.05646789 0.05413332 0.00059251]\n",
      " [0.05649854 0.05415381 0.00065543]\n",
      " [0.05644939 0.05409127 0.0005348 ]\n",
      " [0.0564698  0.0540982  0.00056513]\n",
      " [0.05644032 0.05410455 0.00051158]\n",
      " [0.05647338 0.05408028 0.00058013]\n",
      " [0.05644092 0.05405022 0.00049297]\n",
      " [0.05644599 0.05412395 0.00053745]\n",
      " [0.0564692  0.05409553 0.00055866]\n",
      " [0.05644046 0.05411808 0.0005279 ]\n",
      " [0.0564735  0.05410883 0.00058116]\n",
      " [0.05642833 0.05411603 0.00050489]\n",
      " [0.05645811 0.05407363 0.0005407 ]\n",
      " [0.05643178 0.05411817 0.00050329]\n",
      " [0.05644634 0.054105   0.0005165 ]\n",
      " [0.0564486  0.05406708 0.00051401]\n",
      " [0.05644179 0.05405898 0.00049892]\n",
      " [0.05643263 0.05407334 0.00048229]\n",
      " [0.05642167 0.05408533 0.00047273]\n",
      " [0.05644222 0.05410312 0.0005215 ]\n",
      " [0.05648573 0.05407504 0.00059277]\n",
      " [0.05647756 0.05411495 0.00059404]\n",
      " [0.05648379 0.05410786 0.00060063]\n",
      " [0.05646725 0.05408922 0.00056358]\n",
      " [0.05642754 0.05413868 0.00050357]\n",
      " [0.05644798 0.05413693 0.00054698]\n",
      " [0.05643918 0.05408607 0.00050951]\n",
      " [0.0564325  0.05406561 0.00048233]\n",
      " [0.05644773 0.05410007 0.0005278 ]\n",
      " [0.05646127 0.05410154 0.00055635]\n",
      " [0.05646538 0.0541243  0.00057861]\n",
      " [0.05645919 0.05413741 0.0005607 ]\n",
      " [0.0564423  0.05409628 0.00051294]\n",
      " [0.05646501 0.05409832 0.00056194]\n",
      " [0.05650367 0.0541279  0.00065646]\n",
      " [0.05645925 0.05411877 0.00055969]\n",
      " [0.05646111 0.05409827 0.00054901]\n",
      " [0.05645795 0.05410408 0.0005498 ]\n",
      " [0.05645054 0.05407851 0.00052272]\n",
      " [0.05650637 0.05409553 0.00065731]\n",
      " [0.05646032 0.0541033  0.00055626]\n",
      " [0.05638786 0.0541477  0.00044476]\n",
      " [0.05642031 0.05415564 0.00050778]\n",
      " [0.05639527 0.05410667 0.00043106]\n",
      " [0.05641136 0.05413685 0.0004702 ]\n",
      " [0.05639692 0.05413558 0.00045152]\n",
      " [0.05638406 0.05410796 0.00040114]\n",
      " [0.05644676 0.05419786 0.00058207]\n",
      " [0.05639582 0.05411065 0.00042001]\n",
      " [0.05640546 0.05413632 0.00045744]\n",
      " [0.05638383 0.0540962  0.00041129]\n",
      " [0.05641336 0.05409806 0.00046993]\n",
      " [0.05641284 0.05412787 0.0004778 ]\n",
      " [0.05640253 0.05410659 0.00045073]\n",
      " [0.05641979 0.05416746 0.00051829]\n",
      " [0.05640611 0.05415917 0.00049901]\n",
      " [0.05640278 0.05411614 0.00046424]\n",
      " [0.05641192 0.05411823 0.00046425]\n",
      " [0.05638176 0.0540853  0.00038591]\n",
      " [0.05637494 0.05412795 0.00039567]\n",
      " [0.05643221 0.05415257 0.00051879]]\n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singh/anaconda3/envs/data/lib/python3.6/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/singh/anaconda3/envs/data/lib/python3.6/site-packages/ipykernel_launcher.py:80: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/singh/anaconda3/envs/data/lib/python3.6/site-packages/ipykernel_launcher.py:80: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/singh/anaconda3/envs/data/lib/python3.6/site-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "# Initialize the neural network model and specify the parameters \n",
    "\n",
    "hidden_nodes=5\n",
    "nnobj= NeuralNetwork(cols,hidden_nodes,3)       \n",
    "epochs = 5\n",
    "learning_rate = 1e-2\n",
    "loss_history = []\n",
    "epoch_history = []\n",
    "\n",
    "# Gradient Descent\n",
    "for e in range(epochs):\n",
    "    yPred = nnobj.forward(X_train)\n",
    "    #print(yPred)\n",
    "    nnobj.backward(X_train, y_train,yPred, lr=learning_rate)   #, lmda=lmda)\n",
    "\n",
    "# yPred = nnobj.forward(X_train)\n",
    "# print(yPred)\n",
    "    \n",
    "# train_loss= crossEntropy_loss(nnobj.forward(X_train), y_train)  #?\n",
    "# train_accuracy= accuracy(nnobj.forward(X_train), y_train) #?\n",
    "# test_loss= crossEntropy_loss(nnobj.forward(X_test), y_test)# ?\n",
    "# test_accuracy= accuracy(nnobj.forward(X_test), y_test) #?\n",
    "    \n",
    "# print(\"Final train_loss \"+ str(train_loss))    \n",
    "# print(\"Final train_accuracy \"+ str(train_accuracy))    \n",
    "# print(\"Testloss \" + str(test_loss))\n",
    "# print(\"Accuracy is \"+ str(test_accuracy))\n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T19:31:28.312719Z",
     "start_time": "2019-01-30T19:31:28.296324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
