{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This particular assignment focuses on text classification using CNN. It has been picking up pace over the past few years. So, I thought this would be a good exercise to try out. The dataset is provided to you and there will be specific instrucions on how to curate the data, split into train and validation and the like.  You will be using MXnet for this task.  The data comprises tweets pertaining to common causes of cancer. The objective is to classify the tweets as medically relevant or not.  The dataset is skewed with positive class or 'yes' being 6 times less frequent than the negative class or 'no'. (Total marks = 50). Individual marks to the sub-problems are given in bracket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T14:25:10.800084Z",
     "start_time": "2019-03-08T14:25:10.791882Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "root_logger = logging.getLogger()\n",
    "# stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "# root_logger.addHandler(stdout_handler)\n",
    "root_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T11:16:27.770782Z",
     "start_time": "2019-03-08T11:16:27.634026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 1298\n"
     ]
    }
   ],
   "source": [
    "# these are the modules you are allowed to work with. \n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import sys, os\n",
    "import random\n",
    "\n",
    "'''\n",
    "First job is to clean and preprocess the social media text. (5)\n",
    "\n",
    "1) Replace URLs and mentions (i.e strings which are preceeded with @)\n",
    "2) Segment #hastags \n",
    "3) Remove emoticons and other unicode characters\n",
    "'''\n",
    "\n",
    "def preprocess_tweet(input_text):\n",
    "    '''\n",
    "    Input: The input string read directly from the file\n",
    "    \n",
    "    Output: Pre-processed tweet text\n",
    "    '''\n",
    "    cleaned_text = re.sub(r\"http\\S+\", \"\", input_text)\n",
    "    cleaned_text = re.sub(r\"@\\S+\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r'[\\U00010000-\\U0010ffff]', \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r'[()]', \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', \"\", cleaned_text).strip().split()\n",
    "    \n",
    "    temp = cleaned_text\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for word in temp:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word[0] == '#':\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            word = word[1 : ]\n",
    "            sp = re.findall(r'[0-9A-Z]?[0-9a-z]+|[A-Z]+(?=[0-9A-Z]|$)', word)\n",
    "            sp = [w.lower() for w in sp]\n",
    "            cleaned_text += sp\n",
    "        if not str.isalnum(word) and not str.isalpha(word):\n",
    "            word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "        else:\n",
    "            cleaned_text.append(word.lower())\n",
    "    \n",
    "    #print(cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# read the input file and create the set of positive examples and negative examples. \n",
    "\n",
    "file=open('cancer_data.tsv')\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    text2 = preprocess_tweet(line[0])\n",
    "    if line[1]=='yes':\n",
    "        pos_data.append(text2)\n",
    "    if line[1]=='no':\n",
    "        neg_data.append(text2)\n",
    "\n",
    "print(len(pos_data), len(neg_data))     \n",
    "\n",
    "sentences= list(pos_data)\n",
    "sentences.extend(neg_data)\n",
    "pos_labels= [1 for _ in pos_data]\n",
    "neg_labels= [0 for _ in neg_data]\n",
    "y=list(pos_labels)\n",
    "y.extend(neg_labels)\n",
    "y=np.array(y)\n",
    "\n",
    "'''\n",
    "After this you will obtain the following :\n",
    "\n",
    "1) sentences =  List of sentences having the positive and negative examples with all the positive examples first\n",
    "2) y = List of labels with the positive labels first.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Before running the CNN there are a few things one needs to take care of: (5)\n",
    "\n",
    "1) Pad the sentences so that all of them are of the same length\n",
    "2) Build a vocabulary comprising all unique words that occur in the corpus\n",
    "3) Convert each sentence into a corresponding vector by replacing each word in the sentence with the index in the vocabulary. \n",
    "\n",
    "Example :\n",
    "S1 = a b a c\n",
    "S2 = d c a \n",
    "\n",
    "Step 1:  S1= a b a c, \n",
    "         S2 =d c a </s> \n",
    "         (Both sentences are of equal length). \n",
    "\n",
    "Step 2:  voc={a:1, b:2, c:3, d:4, </s>: 5}\n",
    "\n",
    "Step 3:  S1= [1,2,1,3]\n",
    "         S2= [4,3,1,5]\n",
    "\n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def create_word_vectors(sentences):\n",
    "    '''\n",
    "    Input: List of sentences\n",
    "    Output: List of word vectors corresponding to each sentence, vocabulary\n",
    "    '''\n",
    "    new_sentence = []\n",
    "    for words in sentences:\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        #words = [porter.stem(word) for word in words]\n",
    "        new_sentence.append(words)\n",
    "        \n",
    "    max_length = max([len(a) for a in new_sentence])\n",
    "    word_vectors = []\n",
    "    vocabulary = {}\n",
    "    count = 1\n",
    "    for sentence in new_sentence:\n",
    "        word_vectors.append(sentence + [\"</s>\"] * (max_length - len(sentence)))\n",
    "        for word in sentence:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = count\n",
    "                count = count + 1\n",
    "    vocabulary[\"</s>\"] = count\n",
    "    wv = []\n",
    "    for word_vector in word_vectors:\n",
    "        wv.append([vocabulary[word] for word in word_vector])\n",
    "    return np.array(wv), vocabulary\n",
    "\n",
    "\n",
    "def create_shuffle(x,y):\n",
    "    '''\n",
    "    Create an equal distribution of the positive and negative examples. \n",
    "    Please do not change this particular shuffling method.\n",
    "    '''\n",
    "    pos_len= len(pos_data)\n",
    "    neg_len= len(neg_data)\n",
    "    pos_len_train= int(0.8*pos_len)\n",
    "    neg_len_train= int(0.8*neg_len)\n",
    "    train_data= [(x[i],y[i]) for i in range(0, pos_len_train)]\n",
    "    train_data.extend([(x[i],y[i]) for i in range(pos_len, pos_len+ neg_len_train )])\n",
    "    test_data=[(x[i],y[i]) for i in range(pos_len_train, pos_len)]\n",
    "    test_data.extend([(x[i],y[i]) for i in range(pos_len+ neg_len_train, len(x) )])\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    x_train=[i[0] for i in train_data]\n",
    "    y_train=[i[1] for i in train_data]\n",
    "    random.shuffle(test_data)\n",
    "    x_test=[i[0] for i in test_data]\n",
    "    y_test=[i[1] for i in test_data]\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    x_test= np.array(x_test)\n",
    "    y_test= np.array(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test= create_shuffle(x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:00.594988Z",
     "start_time": "2019-03-08T12:01:00.550416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1506, 68)\n",
      "(1204, 68)\n"
     ]
    }
   ],
   "source": [
    "x, vocabulary = create_word_vectors(sentences)\n",
    "print(x.shape)\n",
    "vocab_size = len(vocabulary)\n",
    "sent_size = x.shape[1]\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:05:15.363555Z",
     "start_time": "2019-03-08T12:05:15.346486Z"
    }
   },
   "outputs": [],
   "source": [
    "def sym_gen(batch_size=20, sentences_size=sent_size, num_embed=200,\n",
    "            num_label=2, filter_list=[2, 3, 4, 5], num_filter=100,\n",
    "            dropout=0.1):\n",
    "    \n",
    "    input_x = mx.sym.Variable('data')\n",
    "    input_y = mx.sym.Variable('softmax_label')\n",
    "\n",
    "    # embedding layer\n",
    "    embed_layer = mx.sym.Embedding(data=input_x,\n",
    "                                   input_dim=vocab_size,\n",
    "                                   output_dim=num_embed,\n",
    "                                   name='vocab_embed')\n",
    "    conv_input = mx.sym.Reshape(data=embed_layer, target_shape=(batch_size, 1, sentences_size, num_embed))\n",
    "\n",
    "    # create convolution + (max) pooling layer for each filter operation\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_list):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "        relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentences_size - filter_size + 1, 1), stride=(1, 1))\n",
    "        pooled_outputs.append(pooli)\n",
    "\n",
    "    # combine all pooled outputs\n",
    "    total_filters = num_filter * len(filter_list)\n",
    "    concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "    h_pool = mx.sym.Reshape(data=concat, target_shape=(batch_size, total_filters))\n",
    "\n",
    "    # dropout layer\n",
    "    if dropout > 0.0:\n",
    "        h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "    else:\n",
    "        h_drop = h_pool\n",
    "\n",
    "    # fully connected\n",
    "    cls_weight = mx.sym.Variable('cls_weight')\n",
    "    cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "    fc = mx.sym.FullyConnected(data=h_drop, weight=cls_weight, bias=cls_bias, num_hidden=2)\n",
    "\n",
    "    # softmax output\n",
    "    sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "    return sm, ('data',), ('softmax_label',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:08:58.647509Z",
     "start_time": "2019-03-08T12:08:58.616192Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(symbol_data, train_iterator, valid_iterator, data_column_names, target_names):\n",
    "    devs = mx.cpu()  # default setting\n",
    "    module = mx.mod.Module(symbol_data, data_names=data_column_names, label_names=target_names, context=devs)\n",
    "    module.fit(train_data=train_iterator,\n",
    "              eval_data=valid_iterator,\n",
    "              eval_metric=\"acc\",\n",
    "              optimizer='RMSProp',\n",
    "              optimizer_params={'learning_rate': 0.005},\n",
    "              initializer=mx.initializer.Xavier(),\n",
    "              num_epoch=10,\n",
    "              batch_end_callback=mx.callback.Speedometer(20, 60))\n",
    "\n",
    "train_set = mx.io.NDArrayIter(x_train, y_train, batch_size=20)\n",
    "test_set = mx.io.NDArrayIter(x_test, y_test, batch_size=20)\n",
    "sym_data, names_data, names_label = sym_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T17:03:05.575116Z",
     "start_time": "2019-03-08T17:02:13.495940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [60]\tSpeed: 258.03 samples/sec\taccuracy=0.886885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Batch [60]\tSpeed: 258.03 samples/sec\taccuracy=0.886885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Train-accuracy=0.886885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Train-accuracy=0.886885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Time cost=4.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Time cost=4.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Validation-accuracy=0.900000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Validation-accuracy=0.900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[1] Batch [60]\tSpeed: 247.41 samples/sec\taccuracy=0.969672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Batch [60]\tSpeed: 247.41 samples/sec\taccuracy=0.969672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[1] Train-accuracy=0.969672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Train-accuracy=0.969672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[1] Time cost=4.904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Time cost=4.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[1] Validation-accuracy=0.893750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Validation-accuracy=0.893750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[2] Batch [60]\tSpeed: 241.46 samples/sec\taccuracy=0.992623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2] Batch [60]\tSpeed: 241.46 samples/sec\taccuracy=0.992623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[2] Train-accuracy=0.992623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2] Train-accuracy=0.992623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[2] Time cost=5.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2] Time cost=5.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[2] Validation-accuracy=0.903125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2] Validation-accuracy=0.903125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[3] Batch [60]\tSpeed: 240.12 samples/sec\taccuracy=0.996721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3] Batch [60]\tSpeed: 240.12 samples/sec\taccuracy=0.996721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[3] Train-accuracy=0.996721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3] Train-accuracy=0.996721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[3] Time cost=5.040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3] Time cost=5.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[3] Validation-accuracy=0.896875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3] Validation-accuracy=0.896875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[4] Batch [60]\tSpeed: 263.11 samples/sec\taccuracy=0.996721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4] Batch [60]\tSpeed: 263.11 samples/sec\taccuracy=0.996721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[4] Train-accuracy=0.996721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4] Train-accuracy=0.996721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[4] Time cost=4.601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4] Time cost=4.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[4] Validation-accuracy=0.900000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4] Validation-accuracy=0.900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[5] Batch [60]\tSpeed: 247.88 samples/sec\taccuracy=0.997541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5] Batch [60]\tSpeed: 247.88 samples/sec\taccuracy=0.997541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[5] Train-accuracy=0.997541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5] Train-accuracy=0.997541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[5] Time cost=4.884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5] Time cost=4.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[5] Validation-accuracy=0.896875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5] Validation-accuracy=0.896875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[6] Batch [60]\tSpeed: 252.02 samples/sec\taccuracy=0.998361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6] Batch [60]\tSpeed: 252.02 samples/sec\taccuracy=0.998361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[6] Train-accuracy=0.998361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6] Train-accuracy=0.998361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[6] Time cost=4.813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6] Time cost=4.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[6] Validation-accuracy=0.903125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6] Validation-accuracy=0.903125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[7] Batch [60]\tSpeed: 274.14 samples/sec\taccuracy=0.998361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7] Batch [60]\tSpeed: 274.14 samples/sec\taccuracy=0.998361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[7] Train-accuracy=0.998361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7] Train-accuracy=0.998361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[7] Time cost=4.419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7] Time cost=4.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[7] Validation-accuracy=0.900000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7] Validation-accuracy=0.900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[8] Batch [60]\tSpeed: 255.46 samples/sec\taccuracy=0.998361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8] Batch [60]\tSpeed: 255.46 samples/sec\taccuracy=0.998361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[8] Train-accuracy=0.998361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8] Train-accuracy=0.998361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[8] Time cost=4.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8] Time cost=4.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[8] Validation-accuracy=0.903125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8] Validation-accuracy=0.903125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[9] Batch [60]\tSpeed: 295.75 samples/sec\taccuracy=0.998361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9] Batch [60]\tSpeed: 295.75 samples/sec\taccuracy=0.998361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[9] Train-accuracy=0.998361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9] Train-accuracy=0.998361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[9] Time cost=4.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9] Time cost=4.097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[9] Validation-accuracy=0.900000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9] Validation-accuracy=0.900000\n"
     ]
    }
   ],
   "source": [
    "model = train(sym_data, train_set, test_set, names_data, names_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T01:29:45.850773Z",
     "start_time": "2019-03-08T01:29:45.832449Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T09:54:29.176096Z",
     "start_time": "2019-05-28T09:54:29.144846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1) Would the results improve if instead of using the word embeddings that is based\\nsolely on frequency, if you have been able to incorporate sub-word information\\n   (In short run fasttext on the corpus and use the word embeddings generated by fasttxt). (8)\\n   \\n2) Accuracy might not be the best way to measure the performance of a skewed dataset. \\nWhat other metrics would you use ? Why? \\n   Experiment with different hyper-paramters to show the performance in terms of metric? \\n   You can assume that we want to identify all the medically relevant tweets (i.e. tweets\\n   with 'yes' class more). (7)\\n    \\n\\nDelivearbles:\\n\\nThe ipython notebook with the results to each part of the question. \\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1) Would the results improve if instead of using the word embeddings that is based\n",
    "solely on frequency, if you have been able to incorporate sub-word information\n",
    "   (In short run fasttext on the corpus and use the word embeddings generated by fasttxt). (8)\n",
    "   \n",
    "2) Accuracy might not be the best way to measure the performance of a skewed dataset. \n",
    "What other metrics would you use ? Why? \n",
    "   Experiment with different hyper-paramters to show the performance in terms of metric? \n",
    "   You can assume that we want to identify all the medically relevant tweets (i.e. tweets\n",
    "   with 'yes' class more). (7)\n",
    "    \n",
    "\n",
    "Delivearbles:\n",
    "\n",
    "The ipython notebook with the results to each part of the question. \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
