{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:16.663329Z",
     "start_time": "2019-03-08T12:01:14.828705Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/singh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import random\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:18.793612Z",
     "start_time": "2019-03-08T12:01:18.782679Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "root_logger = logging.getLogger()\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "root_logger.addHandler(stdout_handler)\n",
    "root_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:19.444821Z",
     "start_time": "2019-03-08T12:01:19.423826Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_tweet(input_text):\n",
    "    '''\n",
    "    Input: The input string read directly from the file\n",
    "    \n",
    "    Output: Pre-processed tweet text\n",
    "    '''   \n",
    "    \n",
    "    link_re = r\"http\\S+\"\n",
    "    mention_re = r\"@\\S+\"\n",
    "    emoji_uni_re = r'[\\U00010000-\\U0010ffff]'\n",
    "    braces_re = r'[()]'\n",
    "    del_re = r'|'.join((link_re, mention_re, emoji_uni_re, braces_re))\n",
    "    ct = re.sub(del_re, '', input_text)\n",
    "    ct = re.sub(r'[^\\x00-\\x7F]+','', ct).strip().split()\n",
    "    cleaned_text = []\n",
    "    for word in ct:\n",
    "        if word[-1] == ')':\n",
    "            word = word[ : -1]\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word[0] == '(':\n",
    "            word = word[1 : ]\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word[0] == '#':\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            word = word[1 : ]\n",
    "            sp = re.findall(r'[0-9A-Z]?[0-9a-z]+|[A-Z]+(?=[0-9A-Z]|$)', word)\n",
    "            sp = [w.lower() for w in sp]\n",
    "            cleaned_text += sp\n",
    "        else:\n",
    "            cleaned_text.append(word.lower())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:20.190043Z",
     "start_time": "2019-03-08T12:01:20.096555Z"
    }
   },
   "outputs": [],
   "source": [
    "file=open('cancer_data.tsv')\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "words = []\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    text2 = preprocess_tweet(line[0])\n",
    "    if line[1] == 'yes':\n",
    "        pos_data.append(text2)\n",
    "    if line[1] == 'no':\n",
    "        neg_data.append(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:20.780405Z",
     "start_time": "2019-03-08T12:01:20.764224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 1298\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_data), len(neg_data))     \n",
    "\n",
    "sentences = list(pos_data)\n",
    "sentences.extend(neg_data)\n",
    "pos_labels = [1 for _ in pos_data]\n",
    "neg_labels = [0 for _ in neg_data]\n",
    "y = list(pos_labels)\n",
    "y.extend(neg_labels)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:21.807085Z",
     "start_time": "2019-03-08T12:01:21.802377Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_word_vectors(sentences):\n",
    "    '''\n",
    "    Input: List of sentences\n",
    "    Output: List of word vectors corresponding to each sentence, vocabulary\n",
    "    '''\n",
    "    max_length = max([len(a) for a in sentences])\n",
    "    word_vectors = []\n",
    "    vocabulary = {}\n",
    "    count = 1\n",
    "    for sentence in sentences:\n",
    "        word_vectors.append(sentence + [\"</s>\"] * (max_length - len(sentence)))\n",
    "        for word in sentence:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = count\n",
    "                count = count + 1\n",
    "    vocabulary[\"</s>\"] = count\n",
    "    wv = []\n",
    "    for word_vector in word_vectors:\n",
    "        wv.append([vocabulary[word] for word in word_vector])\n",
    "    return np.array(wv), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:43.194738Z",
     "start_time": "2019-03-08T12:01:43.118984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1506, 118)\n",
      "(1204, 118)\n"
     ]
    }
   ],
   "source": [
    "x, vocabulary = create_word_vectors(sentences)\n",
    "print(x.shape)\n",
    "vocab_size = len(vocabulary)\n",
    "sent_size = x.shape[1]\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T12:01:39.214165Z",
     "start_time": "2019-03-08T12:01:39.185375Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_shuffle(x,y):\n",
    "    '''\n",
    "    Create an equal distribution of the positive and negative examples. \n",
    "    Please do not change this particular shuffling method.\n",
    "    '''\n",
    "    pos_len= len(pos_data)\n",
    "    neg_len= len(neg_data)\n",
    "    pos_len_train= int(0.8*pos_len)\n",
    "    neg_len_train= int(0.8*neg_len)\n",
    "    train_data= [(x[i],y[i]) for i in range(0, pos_len_train)]\n",
    "    train_data.extend([(x[i],y[i]) for i in range(pos_len, pos_len+ neg_len_train )])\n",
    "    test_data=[(x[i],y[i]) for i in range(pos_len_train, pos_len)]\n",
    "    test_data.extend([(x[i],y[i]) for i in range(pos_len+ neg_len_train, len(x) )])\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    x_train=[i[0] for i in train_data]\n",
    "    y_train=[i[1] for i in train_data]\n",
    "    random.shuffle(test_data)\n",
    "    x_test=[i[0] for i in test_data]\n",
    "    y_test=[i[1] for i in test_data]\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    x_test= np.array(x_test)\n",
    "    y_test= np.array(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test= create_shuffle(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sym(sent_size, embed_size=200, filter_list=[2, 3, 4, 5], num_filter=100, dropout=0.0, batch_size=20) :\n",
    "    input_x = mx.sym.Variable('data')\n",
    "    input_y = mx.sym.Variable('softmax_label')\n",
    "    embed_layer = mx.sym.Embedding(data=input_x,\n",
    "                                  input_dim=vocab_size,\n",
    "                                  output_dim=embed_size,\n",
    "                                  name='vocab_embed')\n",
    "    conv_input = mx.sym.Reshape(data=embed_layer,\n",
    "                               target_shape=(batch_size, 1, sent_size, embed_size))\n",
    "    pool_outs = []\n",
    "    for i, filter_size in enumerate(filter_list):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, embed_size), num_filter=num_filter)\n",
    "        relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sent_size - filter_size + 1, 1), stride=(1, 1))\n",
    "        pool_outs.append(pooli)\n",
    "    tot_filters = num_filter * len(filter_list)\n",
    "    concat = mx.sym.Concat(*pool_outs, dim=1)\n",
    "    h_pool = mx.sym.Reshape(data = concat, target_shape=(batch_size, tot_filters))\n",
    "    if dropout > 0.0:\n",
    "        h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "    else:\n",
    "        h_drop = h_pool\n",
    "    \n",
    "    fc_weight = mx.sym.Variable('fc_weight')\n",
    "    fc_bias = mx.sym.Variable('fc_bias')\n",
    "    \n",
    "    fc = mx.sym.FullyConnected(data=h_drop, weight=fc_weight, bias=fc_bias, num_hidden=2)\n",
    "    \n",
    "    sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "    \n",
    "    return sm, ('data', ), ('softmax_label', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sym_data, train_iterator, test_iterator, names_data, names_label):\n",
    "    module = mx.mod.Module(sym_data, data_names=names_data, label_names=names_label)\n",
    "    module.fit(train_data=train_iterator,\n",
    "              eval_data=test_iterator,\n",
    "              eval_metric=\"acc\",\n",
    "              optimizer='RMSProp',\n",
    "              optimizer_params={'learning_rate': 0.005},\n",
    "              initializer=mx.initializer.Xavier(),\n",
    "              num_epoch=10,\n",
    "              batch_end_callback=mx.callback.Speedometer(20, 60)\n",
    "              )\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = mx.io.NDArrayIter(x_train, y_train, batch_size=20)\n",
    "test_set = mx.io.NDArrayIter(x_test, y_test, batch_size=20)\n",
    "sym_data, names_data, names_label = create_sym(sent_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Batch [60]\tSpeed: 168.85 samples/sec\taccuracy=0.868852\n",
      "Epoch[0] Batch [60]\tSpeed: 168.85 samples/sec\taccuracy=0.868852\n",
      "Epoch[0] Train-accuracy=0.868852\n",
      "Epoch[0] Train-accuracy=0.868852\n",
      "Epoch[0] Time cost=7.214\n",
      "Epoch[0] Time cost=7.214\n",
      "Epoch[0] Validation-accuracy=0.890625\n",
      "Epoch[0] Validation-accuracy=0.890625\n",
      "Epoch[1] Batch [60]\tSpeed: 162.77 samples/sec\taccuracy=0.952459\n",
      "Epoch[1] Batch [60]\tSpeed: 162.77 samples/sec\taccuracy=0.952459\n",
      "Epoch[1] Train-accuracy=0.952459\n",
      "Epoch[1] Train-accuracy=0.952459\n",
      "Epoch[1] Time cost=7.436\n",
      "Epoch[1] Time cost=7.436\n"
     ]
    }
   ],
   "source": [
    "model = train(sym_data, train_set, test_set, names_data, names_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outs = model.predict(train_set)\n",
    "test_outs = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = 100 * sum(y_train == np.argmax(train_outs, axis=1).asnumpy()) / len(y_train) \n",
    "test_acc = 100 * sum(y_test == np.argmax(test_outs, axis=1).asnumpy()) / len(y_test) \n",
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sym_ft(sent_size, embed_size=200, filter_list=[2, 3, 4, 5], num_filter=100, dropout=0.0, batch_size=20) :\n",
    "    input_x = mx.sym.Variable('data')\n",
    "    input_y = mx.sym.Variable('softmax_label')\n",
    "    embed_layer = mx.contrib.text.embedding.create('fasttext', pretrained_file_name='wiki.simple.vec', vocabulary=vocabulary)\n",
    "    conv_input = mx.sym.Reshape(data=embed_layer,\n",
    "                               target_shape=(batch_size, 1, sent_size, embed_size))\n",
    "    pool_outs = []\n",
    "    for i, filter_size in enumerate(filter_list):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, embed_size), num_filter=num_filter)\n",
    "        relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sent_size - filter_size + 1, 1), stride=(1, 1))\n",
    "        pool_outs.append(pooli)\n",
    "    tot_filters = num_filter * len(filter_list)\n",
    "    concat = mx.sym.Concat(*pool_outs, dim=1)\n",
    "    h_pool = mx.sym.Reshape(data = concat, target_shape=(batch_size, tot_filters))\n",
    "    if dropout > 0.0:\n",
    "        h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "    else:\n",
    "        h_drop = h_pool\n",
    "    \n",
    "    fc_weight = mx.sym.Variable('fc_weight')\n",
    "    fc_bias = mx.sym.Variable('fc_bias')\n",
    "    \n",
    "    fc = mx.sym.FullyConnected(data=h_drop, weight=fc_weight, bias=fc_bias, num_hidden=2)\n",
    "    \n",
    "    sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "    \n",
    "    return sm, ('data', ), ('softmax_label', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib import text\n",
    "import itertools\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(itertools.chain(*sentences))\n",
    "my_vocab = text.vocab.Vocabulary(counts)\n",
    "my_embedding = text.embedding.create('fasttext', pretrained_file_name='wiki.simple.vec', vocabulary=my_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = [my_embedding.get_vecs_by_tokens(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = np.array(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'[()]', '','I wanted to (say) something(s) (but I cannot)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
